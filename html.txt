


















Graphormer




 Hugging Face        Models Datasets Spaces Docs  Solutions   Pricing     Log In Sign Up




Transformers documentation
			
Graphormer





					Transformers
					



Search documentation


mainv4.34.0v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html
DEENESFRITKOPTZH








Get started


ü§ó Transformers
Quick tour
Installation


Tutorials


Run inference with pipelines
Write portable code with AutoClass
Preprocess data
Fine-tune a pretrained model
Train with a script
Set up distributed training with ü§ó Accelerate
Load and train adapters with ü§ó PEFT
Share your model
Agents
Generation with LLMs


Task Guides



Natural Language Processing


Audio


Computer Vision


Multimodal


Generation


Prompting



Developer guides


Use fast tokenizers from ü§ó Tokenizers
Run inference with multilingual models
Use model-specific APIs
Share a custom model
Templates for chat models
Run training on Amazon SageMaker
Export to ONNX
Export to TFLite
Export to TorchScript
Benchmarks
Notebooks with examples
Community resources
Custom Tools and Prompts
Troubleshoot


Performance and scalability


Overview

Efficient training techniques


Methods and tools for efficient training on a single GPU
Multiple GPUs and parallelism
Efficient training on CPU
Distributed CPU training
Training on TPUs
Training on TPU with TensorFlow
Training on Specialized Hardware
Custom hardware for training
Hyperparameter Search using Trainer API


Optimizing inference


Inference on CPU
Inference on one GPU
Inference on many GPUs
Inference on Specialized Hardware

Instantiating a big model
Troubleshooting
XLA Integration for TensorFlow Models
Optimize inference using `torch.compile()`


Contribute


How to contribute to transformers?
How to add a model to ü§ó Transformers?
How to convert a ü§ó Transformers model to TensorFlow?
How to add a pipeline to ü§ó Transformers?
Testing
Checks on a Pull Request


Conceptual guides


Philosophy
Glossary
What ü§ó Transformers can do
How ü§ó Transformers solve tasks
The Transformer model family
Summary of the tokenizers
Attention mechanisms
Padding and truncation
BERTology
Perplexity of fixed-length models
Pipelines for webserver inference
Model training anatomy


API



Main Classes


Agents and Tools
Auto Classes
Callbacks
Configuration
Data Collator
Keras callbacks
Logging
Models
Text Generation
ONNX
Optimization
Model outputs
Pipelines
Processors
Quantization
Tokenizer
Trainer
DeepSpeed Integration
Feature Extractor
Image Processor


Models



Text models


Vision models


Audio models


Multimodal models


Reinforcement learning models


Time series models


Graph models


Graphormer



Internal Helpers


Custom Layers and Utilities
Utilities for pipelines
Utilities for Tokenizers
Utilities for Trainer
Utilities for Generation
Utilities for Image Processors
Utilities for Audio processing
General Utilities
Utilities for Time Series





Join the Hugging Face community
and get access to the augmented documentation experience
		

Collaborate on models, datasets and Spaces
				

Faster examples with accelerated inference
				

Switch between documentation themes
				
Sign Up
to get started
 












   Graphormer  Overview The Graphormer model was proposed in Do Transformers Really Perform Bad for Graph Representation?  by
Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen and Tie-Yan Liu. It is a Graph Transformer model, modified to allow computations on graphs instead of text sequences by generating embeddings and features of interest during preprocessing and collation, then using a modified attention. The abstract from the paper is the following: The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer. Tips: This model will not work well on large graphs (more than 100 nodes/edges), as it will make the memory explode.
You can reduce the batch size, increase your RAM, or decrease the UNREACHABLE_NODE_DISTANCE parameter in algos_graphormer.pyx, but it will be hard to go above 700 nodes/edges. This model does not use a tokenizer, but instead a special collator during training. This model was contributed by clefourrier. The original code can be found here.  GraphormerConfig  class transformers.GraphormerConfig  < source > ( num_classes: int = 1 num_atoms: int = 4608 num_edges: int = 1536 num_in_degree: int = 512 num_out_degree: int = 512 num_spatial: int = 512 num_edge_dis: int = 128 multi_hop_max_dist: int = 5 spatial_pos_max: int = 1024 edge_type: str = 'multi_hop' max_nodes: int = 512 share_input_output_embed: bool = False num_hidden_layers: int = 12 embedding_dim: int = 768 ffn_embedding_dim: int = 768 num_attention_heads: int = 32 dropout: float = 0.1 attention_dropout: float = 0.1 activation_dropout: float = 0.1 layerdrop: float = 0.0 encoder_normalize_before: bool = False pre_layernorm: bool = False apply_graphormer_init: bool = False activation_fn: str = 'gelu' embed_scale: float = None freeze_embeddings: bool = False num_trans_layers_to_freeze: int = 0 traceable: bool = False q_noise: float = 0.0 qn_block_size: int = 8 kdim: int = None vdim: int = None bias: bool = True self_attention: bool = True pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 **kwargs  )   Parameters   num_classes (int, optional, defaults to 1) ‚Äî
Number of target classes or labels, set to n for binary classification of n tasks.   num_atoms (int, optional, defaults to 512*9) ‚Äî
Number of node types in the graphs.   num_edges (int, optional, defaults to 512*3) ‚Äî
Number of edges types in the graph.   num_in_degree (int, optional, defaults to 512) ‚Äî
Number of in degrees types in the input graphs.   num_out_degree (int, optional, defaults to 512) ‚Äî
Number of out degrees types in the input graphs.   num_edge_dis (int, optional, defaults to 128) ‚Äî
Number of edge dis in the input graphs.   multi_hop_max_dist (int, optional, defaults to 20) ‚Äî
Maximum distance of multi hop edges between two nodes.   spatial_pos_max (int, optional, defaults to 1024) ‚Äî
Maximum distance between nodes in the graph attention bias matrices, used during preprocessing and
collation.   edge_type (str, optional, defaults to multihop) ‚Äî
Type of edge relation chosen.   max_nodes (int, optional, defaults to 512) ‚Äî
Maximum number of nodes which can be parsed for the input graphs.   share_input_output_embed (bool, optional, defaults to False) ‚Äî
Shares the embedding layer between encoder and decoder - careful, True is not implemented.   num_layers (int, optional, defaults to 12) ‚Äî
Number of layers.   embedding_dim (int, optional, defaults to 768) ‚Äî
Dimension of the embedding layer in encoder.   ffn_embedding_dim (int, optional, defaults to 768) ‚Äî
Dimension of the ‚Äúintermediate‚Äù (often named feed-forward) layer in encoder.   num_attention_heads (int, optional, defaults to 32) ‚Äî
Number of attention heads in the encoder.   self_attention (bool, optional, defaults to True) ‚Äî
Model is self attentive (False not implemented).   activation_function (str or function, optional, defaults to "gelu") ‚Äî
The non-linear activation function (function or string) in the encoder and pooler. If string, "gelu",
"relu", "silu" and "gelu_new" are supported.   dropout (float, optional, defaults to 0.1) ‚Äî
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.   attention_dropout (float, optional, defaults to 0.1) ‚Äî
The dropout probability for the attention weights.   activation_dropout (float, optional, defaults to 0.1) ‚Äî
The dropout probability for the activation of the linear transformer layer.   layerdrop (float, optional, defaults to 0.0) ‚Äî
The LayerDrop probability for the encoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)
for more details.   bias (bool, optional, defaults to True) ‚Äî
Uses bias in the attention module - unsupported at the moment.   embed_scale(float, optional, defaults to None) ‚Äî
Scaling factor for the node embeddings.   num_trans_layers_to_freeze (int, optional, defaults to 0) ‚Äî
Number of transformer layers to freeze.   encoder_normalize_before (bool, optional, defaults to False) ‚Äî
Normalize features before encoding the graph.   pre_layernorm (bool, optional, defaults to False) ‚Äî
Apply layernorm before self attention and the feed forward network. Without this, post layernorm will be
used.   apply_graphormer_init (bool, optional, defaults to False) ‚Äî
Apply a custom graphormer initialisation to the model before training.   freeze_embeddings (bool, optional, defaults to False) ‚Äî
Freeze the embedding layer, or train it along the model.   encoder_normalize_before (bool, optional, defaults to False) ‚Äî
Apply the layer norm before each encoder block.   q_noise (float, optional, defaults to 0.0) ‚Äî
Amount of quantization noise (see ‚ÄúTraining with Quantization Noise for Extreme Model Compression‚Äù). (For
more detail, see fairseq‚Äôs documentation on quant_noise).   qn_block_size (int, optional, defaults to 8) ‚Äî
Size of the blocks for subsequent quantization with iPQ (see q_noise).   kdim (int, optional, defaults to None) ‚Äî
Dimension of the key in the attention, if different from the other values.   vdim (int, optional, defaults to None) ‚Äî
Dimension of the value in the attention, if different from the other values.   use_cache (bool, optional, defaults to True) ‚Äî
Whether or not the model should return the last key/values attentions (not used by all models).   traceable (bool, optional, defaults to False) ‚Äî
Changes return value of the encoder‚Äôs inner_state to stacked tensors.
Example ‚Äî   This is the configuration class to store the configuration of a ~GraphormerModel. It is used to instantiate an
Graphormer model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the Graphormer
graphormer-base-pcqm4mv1 architecture. Configuration objects inherit from PretrainedConfig and can be used to control the model outputs. Read the
documentation from PretrainedConfig for more information.  GraphormerModel  class transformers.GraphormerModel  < source > ( config: GraphormerConfig  )    The Graphormer model is a graph-encoder model. It goes from a graph to its representation. If you want to use the model for a downstream classification task, use
GraphormerForGraphClassification instead. For any other downstream task, feel free to add a new class, or combine
this model with a downstream model of your choice, following the example in GraphormerForGraphClassification.  forward  < source > ( input_nodes: LongTensor input_edges: LongTensor attn_bias: Tensor in_degree: LongTensor out_degree: LongTensor spatial_pos: LongTensor attn_edge_type: LongTensor perturb: typing.Optional[torch.FloatTensor] = None masked_tokens: None = None return_dict: typing.Optional[bool] = None **unused  )     GraphormerForGraphClassification  class transformers.GraphormerForGraphClassification  < source > ( config: GraphormerConfig  )    This model can be used for graph-level classification or regression tasks. It can be trained on regression (by setting config.num_classes to 1); there should be one float-type label per graph one task classification (by setting config.num_classes to the number of classes); there should be one integer
label per graph binary multi-task classification (by setting config.num_classes to the number of labels); there should be a list
of integer labels for each graph.  forward  < source > ( input_nodes: LongTensor input_edges: LongTensor attn_bias: Tensor in_degree: LongTensor out_degree: LongTensor spatial_pos: LongTensor attn_edge_type: LongTensor labels: typing.Optional[torch.LongTensor] = None return_dict: typing.Optional[bool] = None **unused  )    


‚ÜêTime Series Transformer
Custom Layers and Utilities‚Üí

Graphormer
Overview
GraphormerConfig
GraphormerModel
GraphormerForGraphClassification










